---
title: "Practical Machine Learning:"
subtitle: "Weight Lifting Exercise Quality Classification"
author: "adatum"
date: "April 25, 2016"
output: 
    html_document:
      theme: cerulean
      toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)

if (!require("pacman")) install.packages("pacman")
pacman::p_load(caret, caretEnsemble, arm, C50, plyr, earth, mda, gbm, MASS, randomForest, sda, kernlab, xgboost)
set.seed(42)
```

### Introduction
With wearable heath and fitness devices becoming increasingly popular, and correspondingly, the amount of data collected by such devices burgeoning, the question of how to make use of these vast stores of information holds much potential. Many devices report simple aggregate summaries of the quantity of particular activities performed, perhaps in relation to some goal. On the other hand, assessing the quality of the activities is more challenging and may represent significant untapped potential hidden in the data.

The [Weight Lifting Exercises Dataset](http://groupware.les.inf.puc-rio.br/har#weight_lifting_exercises) aims to test the feasibility of assessing the quality of weight lifting exercises performed by inexperienced individuals, potentially as a precursor to personalized digital personal trainers. The data were collected by attaching orientation sensors on a dumbbell and the participant's arm, forearm, and belt. They were then instructed to perform 10 sets of unilateral dumbbell biceps curls in one correct and four incorrect ways: "*exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E).*" More details are available at the [researchers' website](http://groupware.les.inf.puc-rio.br/har).

The goal is to develop machine learning algorithms which correctly classify the quality of the exercise performed given the sensor data.

### Data Exploration and Preprocessing

The training and testing data sets are downloaded and read, if not already present.

```{r import-data}
if(!exists("pml_training") & !exists("pml_testing")){
        training_file <- "pml-training.csv"
        testing_file <- "pml-testing.csv"
        
        if(!file.exists(training_file) & !file.exists(testing_file)){
        training_url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
        testing_url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
        download.file(training_url, training_file, method = "curl")
        download.file(testing_url, testing_file, method = "curl")
        }
        
        pml_training <- read.csv("pml-training.csv", na.strings = c(NA, ""))
        pml_testing <- read.csv("pml-testing.csv", na.strings = c(NA, ""))
}

dim(pml_training)
names(pml_training)
```

To begin, the data are partitioned into training and validation sets. 

```{r partition-data}
ind_training <- createDataPartition(pml_training$classe, p = 0.75, list = F)
training <- pml_training[ind_training, ]
validation <- pml_training[-ind_training, ]

str(training, list.len = 15)
```

A few strategic desicions are made in treating the data. First, we notice that several variables are very sparsely populated. We could impute missing values if there are small gaps, but if the ratio of missing values to non-missing values is high, we will prefer to omit those variables.

```{r missing-values}
rows <- nrow(training)
(na_ratio <- sapply(1:ncol(training), function(x) sum(is.na(training[ , x]))/rows))
(nonsparse <- na_ratio < 0.8)
sum(nonsparse)
```

There are `r ncol(training) - sum(nonsparse)` sparse variables. Eliminating them should make our models simpler and more computationally efficient.

Secondly, although the data were clearly collected as a time series, as the experimental design and timestamp variables show, we will initially attempt a simplification: discarding all time and individual participant information (the first 7 columns/variables in the data), and instead train our algorithms on the snapshot-like non-sequential view of the data. In the event that this approach fails to produce a sufficient level of performance, a time series approach can be adopted.

We implement a custom preprocessing function to keep only the non-sparse variables, eliminate the time and individual data, and move the response variable to the first column for convenience.

```{r preproc}

myPreProc <- function(df, keep.cols, skip.cols = 7) {
    
    proc_df <- df[ , keep.cols][ , -(1:skip.cols)]

    # put response variable in first column
    ncols <- ncol(proc_df)
    proc_df <- proc_df[, c(ncols, 1:(ncols-1))]

    # make all columns (except response) numeric
    proc_df[ , 2:ncols] <- as.data.frame(lapply(proc_df[ , -1], as.numeric))
    
    proc_df
}

pr_training <- myPreProc(training, nonsparse)
pr_validation <- myPreProc(validation, nonsparse)
pr_testing <- myPreProc(pml_testing, nonsparse)

```

The exact same preprocessing that is done to the training set is also done to the validation and testing sets. Note that the testing set does not contain the response variable (correct classes) but a placeholder index which will ultimately be ignored.

### Modeling

One approach to modeling is to try many different types of machine learning algorithms initially, evaluate their performance, and then tune the more successful one(s) as needed. This has the benefit of being data-driven based on empirical results. We will try ten algorithms, several of which use very different underlying approaches to the modeling. The `caretEnsemble` package makes it easy to train many models simultaneously with `caretList`.

5-fold cross-validation is used for choosing model parameters. The training data set is split into 5 stratified groups, with the model trained on 4 of them, and the model's performance tested on the remaining group. The groups are then rotated until each group has been used for testing. Often for better model performance, 10-fold repeated cross-validation is suggested, however, here we begin with 5-fold non-repeated cross-validation in the interest of computational speed, and we will consider more stringent methods if the results indicate the need. `caret` automatically tries several parameter combinations (as required by the model) to build the optimal model. If this fails to produce good results we can later specify the number and values of parameters to try.

```{r model-train}
mycontrol <- trainControl(method = "cv", number = 5)
mymethods <- c("bayesglm", "C5.0", "fda", "gbm", "knn",  "qda", "rf", "sda", "svmPoly", "xgbLinear")
    
# load models if already trained since training can take several hours
# if models must be changed, delete "model_list.rds" in working directory

model_fname <- "model_list.rds"
if(file.exists(model_fname)){
    
    model_list <- readRDS(model_fname)
    
} else {

    start_time <- proc.time()
    model_list <- caretList(x = pr_training[ , -1],
                            y = pr_training[ , 1],
                            trControl = mycontrol,
                            methodList = mymethods
                            )
    
    # model training runtime for curiosity
    proc.time() - start_time 

    #save models to speed up 
    saveRDS(model_list, model_fname)
}

dotplot(resamples(model_list))
```

The plot shows promising performance for several of the models.

Next, the built models are used to make predictions on the validation set to obtain our best unbiased estimates of model performance on unseen data. This also helps to make sure we do not have high variance (overfitting) issues. These results guide the decision of which model(s) to use in the future.

```{r model-performance}
pred_list <- lapply(model_list, function(x) predict(x, newdata = pr_validation[ , -1]))
(accuracy <- sort(sapply(pred_list, function(x) confusionMatrix(x, pr_validation[ , 1])$overall[1]), decreasing = T))

```

Several of the models have high accuracies, with `r print(unlist(strsplit(names(accuracy[1]), split = "\\."))[1], quote = F)` having the highest. Furthermore, the confusion matrix indicates the accuracy is consistently high across all classes. Interestingly, it performs perfectly on Class A, which is the class for the weight lifting exercise performed correctly. This means that when an individual performs the exercise well, it is very unlikely for the model to flag it as incorrect, which would be quite frustrating for the user.

```{r model-confusionmatrix}
confusionMatrix(pred_list$xgbLinear, pr_validation[ , 1])
```

### Results

Considering the high accuracies achieved we will apply the models to the test set. Instead of picking only one model with the highest accuracy, it is interesting to see to what extent the different models agree with each other. We can also use an simple ensembling strategy of unweighted votes to make more robust predictions.

```{r model-results}
results <- as.data.frame(lapply(model_list, function(x) predict(x, newdata = pr_testing[ , -1])))
results$ensemble <- apply(results, 1, function(x) names(which.max(table(x))))
results
```

The ensemble result is used as our predictions for the test data set. We see that `r sum(sapply(results[, -11], function(x) all(x == results$ensemble)))` of the models agree completely with each other and the ensemble result, which gives us more confidence in the predictions. 

### Conclusions

We have have deployed ten machine learning algorithms using motion sensor data to classify the quality and correctness of exercises performed by individuals. The best performing model was `r print(unlist(strsplit(names(accuracy[1]), split = "\\."))[1], quote = F)` with `r round(accuracy[[1]], 3)*100`% accuracy. The predictions from all ten models were ensembled together for a more robust performance. 

Although the performance is sufficient for this application, a more thorough exploration of the models' parameters could possibly further improve performance and/or reduce computational time by finding equivalently performing simpler models or improving convergence rates. Also, ensembling could use other strategies such as averaging the prediction probabilities from the individual models. Model stacking is also an option, applying another machine learning algorithm to the data set of individual model results to obtain the final predictions.


### References
Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. [Qualitative Activity Recognition of Weight Lifting Exercises](http://groupware.les.inf.puc-rio.br/work.jsf?p1=11201). Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.
